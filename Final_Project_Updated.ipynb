{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnahitShekikyan/ADS-505-Final-Team-Project/blob/main/Final_Project_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Credit Card Fraud Detection Project**\n",
        "\n",
        "## **Business Problem**\n",
        "The objective of this project is to identify fraudulent credit card transactions. Credit card fraud detection is critical for financial institutions, as it helps prevent financial loss and maintain customer trust. We aim to build a model that can accurately detect fraudulent transactions, which account for only 0.17% of all transactions in our dataset.\n",
        "\n",
        "## **Dataset Information**\n",
        "The dataset contains 284,807 transactions, with 31 features describing each transaction. The target variable indicates whether the transaction is fraudulent (1) or legitimate (0). Given the class imbalance, specific techniques will be applied to address this challenge.\n",
        "    "
      ],
      "metadata": {
        "id": "_RUuTyQtutGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Data & Libraries**"
      ],
      "metadata": {
        "id": "mwVAS-3tjzRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install dmba\n",
        "\n",
        "#library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.decomposition import PCA\n",
        "from dmba import classificationSummary\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UtSI9IjukFQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "lEl8FL_83KZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset in Google Drive\n",
        "data = '/content/drive/MyDrive/creditcard.csv'\n",
        "\n",
        "# Load the CSV into a pandas DataFrame\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "WS37yPJE4RO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Data Information**"
      ],
      "metadata": {
        "id": "xx1ioaVct-7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZLqvdafsATA"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if there are there duplicates?\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "ZHnwLtcf4q87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the class distribution to visualize the class imbalance\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('Distribution of Legitimate vs Fraudulent Transactions')\n",
        "plt.show()\n",
        "\n",
        "# Cheking if there is there class imbalance?\n",
        "df['Class'].value_counts()"
      ],
      "metadata": {
        "id": "AU0-mo4XwpXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram further confirms the extreme imbalance in the dataset with very few fraudulent instances compared to the legitimate ones. We need to handle the class imbalance and do dditional exploratory data analysis focusing on the minority class (fraud) to extract more relevant insights or patterns specific to those cases.\n"
      ],
      "metadata": {
        "id": "jr2lUDGihHy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the transaction amounts for fraud vs non-fraud\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Class', y='Amount', data=df)\n",
        "\n",
        "# Limiting y-axis to focus on smaller amounts for clearer visualization\n",
        "plt.ylim(0, 500)\n",
        "plt.title('Transaction Amounts by Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Q_fzAT3wpTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot indicates that fraudulent transactions (Class 1) have higher median transaction amounts compared to legitimate transactions (Class 0). However, both classes show a wide range of transaction amounts, and there are more outliers in the legitimate transactions, so we need to use the \"Amount\" feature as a predictor since there appears to be a noticeable difference between classes. Investigate the outliers in the legitimate transactions to determine if they might be misclassified frauds or anomalies that need special handling, and normalize or standardize the \"Amount\" variable since it may vary significantly and could influence model performance if left unscaled."
      ],
      "metadata": {
        "id": "qSAcDb3rU-C3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Quality Report**\n"
      ],
      "metadata": {
        "id": "tCBi3qisRVJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary stats\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "yvaXCs9twpQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_quality_report(df):\n",
        "\n",
        "    # Initializing the report dictionary\n",
        "    report = pd.DataFrame(index=df.columns)\n",
        "\n",
        "    # Data types\n",
        "    report['Data Type'] = df.dtypes\n",
        "\n",
        "    # Counting missing values\n",
        "    report['Missing Values'] = df.isnull().sum()\n",
        "\n",
        "    # Counting percentage of missing values\n",
        "    report['% Missing'] = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "    # Counting of unique values\n",
        "    report['Unique Values'] = df.nunique()\n",
        "\n",
        "    # Continuous features summary (only for float64 and int64)\n",
        "    report['Min'] = df.min()\n",
        "    report['Max'] = df.max()\n",
        "    report['Mean'] = df.mean()\n",
        "    report['Median'] = df.median()\n",
        "    report['Standard Deviation'] = df.std()\n",
        "\n",
        "    # Checking for duplicates\n",
        "    report['Duplicates'] = df.duplicated().sum()\n",
        "\n",
        "    # Determine cardinality for categorical variables (assumes non-continuous variables)\n",
        "    report['Cardinality'] = [df[col].nunique() if df[col].dtype == 'object' else 'N/A' for col in df.columns]\n",
        "\n",
        "    # Return the report\n",
        "    return report\n",
        "\n",
        "# Generating the Data Quality Report for the dataset\n",
        "report = data_quality_report(df)\n",
        "\n",
        "# Display the report\n",
        "report"
      ],
      "metadata": {
        "id": "Zw2I3hPqxnLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a detailed summary of each feature in the dataset, covering various statistical and descriptive properties that are crucial for data exploration and preparation.\n",
        "\n",
        "\n",
        "*   **Data Type:** All features, except the target variable (Class), are continuous and represented as floats.\n",
        "\n",
        "*   **Missing Values:** There are no missing values, which indicates the dataset is complete and requires no imputation.\n",
        "\n",
        "\n",
        "*   **Unique Values:** The number of distinct values in each feature. High cardinality (e.g., for Time and most other features) indicates a diverse set of values, typical for continuous variables.\n",
        "\n",
        "\n",
        "*   **Min/Max:** It provides insight into the range of each feature. For example, the feature Amount ranges from 0 to 25691.16, which is consistent with transaction values.\n",
        "\n",
        "*   **Mean/Median:**  This show that most features have means close to zero. This suggests that the features might have been transformed (e.g., PCA) to center their distributions.\n",
        "\n",
        "\n",
        "*   **Standard Deviation:** It show the spread or dispersion of values within each feature. Features like \"Amount\" have a high standard deviation (250.12), reflecting a wide range of transaction values.\n",
        "\n",
        "*   **Duplicates:** There are 1081 duplicate rows in the dataset, which may need to be removed or might prevent bias in model training.\n",
        "\n",
        "\n",
        "*   **Cardinality:** Not applicable for continuous features.\n",
        "\n",
        "Based on the data quality report we need to remove duplicates, handle outliers, scale the data, and  reduce multicollinearity."
      ],
      "metadata": {
        "id": "geHcd7ma2V-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**\n",
        "\n",
        "**For Continuous Features**"
      ],
      "metadata": {
        "id": "u5H50WZZe3Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating summary statistics for continuous variables\n",
        "continuous_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "print(\"Summary statistics for continuous features:\")\n",
        "print(df[continuous_features].describe())\n",
        "\n",
        "# Visualizing continuous variables in sets of 5 per row\n",
        "n_features = len(continuous_features)\n",
        "n_cols = 5  # Number of plots per row\n",
        "\n",
        "for i in range(0, n_features, n_cols):\n",
        "    plt.figure(figsize=(15, 4))  # Adjust the width for 3 plots per row\n",
        "    for j in range(n_cols):\n",
        "        if i + j < n_features:\n",
        "            feature = continuous_features[i + j]\n",
        "\n",
        "            # Create subplot for histogram and boxplot\n",
        "            plt.subplot(1, n_cols, j + 1)\n",
        "            sns.histplot(df[feature], bins=30, kde=True)\n",
        "            plt.title(f'Distribution of {feature}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout for better spacing\n",
        "    plt.show()\n",
        "\n",
        "    # Second row with boxplots for the same features\n",
        "    plt.figure(figsize=(15, 4))  # Separate figure for boxplots\n",
        "    for j in range(n_cols):\n",
        "        if i + j < n_features:\n",
        "            feature = continuous_features[i + j]\n",
        "\n",
        "            # Create subplot for boxplot\n",
        "            plt.subplot(1, n_cols, j + 1)\n",
        "            sns.boxplot(x=df[feature])\n",
        "            plt.title(f'Boxplot of {feature}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout for better spacing\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7kLUhYYsxufI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a histograms and boxplots for each continuous feature to explore their distributions.\n",
        "\n",
        "Histograms show the distribution of data within each feature. It helps identify skewness, modality (e.g., unimodal, bimodal), and if the data follows a normal distribution. For an example features like V1, V2, and others often show normal-like distributions centered around zero. This indicates they may have been scaled or transformed.\n",
        "\n",
        "Boxplots display the spread and presence of outliers for each feature, and gave a view of the median, quartiles, and extreme values. For an exzamle of boxplots of features like V5, V6, etc., reveal many outliers, which can be important when deciding on preprocessing techniques, like applying robust scaling or addressing extreme values to avoid undue influence on the models.\n",
        "\n",
        "Both histograms and boxplots are displayed side by side for each set of features, ensuring that the visual exploration of the dataset is comprehensive."
      ],
      "metadata": {
        "id": "40zMw-AHzjxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Categorical Features**"
      ],
      "metadata": {
        "id": "Z0WmUH1NyFBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the target variable 'Class'\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('Distribution of Class (Fraud vs Non-Fraud)')\n",
        "plt.show()\n",
        "\n",
        "# Display percentage of fraud vs non-fraud transactions\n",
        "class_counts = df['Class'].value_counts(normalize=True) * 100\n",
        "print(\"Percentage distribution of the target variable 'Class':\")\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "ldnzqRM-yGRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart displays the distribution of the target variable, Class, which represents legitimate transactions (Class 0) and fraudulent transactions (Class 1). It shows a significant imbalance between the two classes, with legitimate transactions making up 99.83% of the dataset, while fraudulent transactions only constitute 0.17%.\n",
        "\n",
        "The extreme class imbalance indicated by the chart requires special consideration in the modeling process to ensure the model's effectiveness in identifying the minority class (fraudulent transactions). Proper resampling, algorithm adjustments, and appropriate evaluation metrics will be crucial to developing an effective fraud detection model.\n"
      ],
      "metadata": {
        "id": "HAcTBk_u9GQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multivariate Analysis**"
      ],
      "metadata": {
        "id": "dC1dUaPfydoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a subset of features to avoid too many plots (e.g., V1 to V5)\n",
        "subset_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'Class']\n",
        "\n",
        "# Creating a pair plot\n",
        "sns.pairplot(df[subset_features], hue='Class', diag_kind='kde', plot_kws={'alpha': 0.3})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y_qnN83sygXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot shows the relationships and scatter distributions of selected features (e.g., V1, V2, etc.) across different classes. There are clusters and some separable patterns, particularly between legitimate and fraudulent classes. Here we need to identify and prioritize features that show clear separability between classes as important predictors for modeling. We might use the PCA if multicollinearity or highly correlated features are identified, and clustering algorithms to validate whether the clusters align with the fraud and non-fraud classes."
      ],
      "metadata": {
        "id": "wG-bSwP4jL6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Ploting the heatmap of the correlation matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False, fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pglbuQeUyoi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This heatmap illustrates the correlation between different features. Some features show strong correlations, which could indicate redundancy or multicollinearity issues. Here are the steps we might go for the next steps:\n",
        "\n",
        "\n",
        "*   Removing or combine highly correlated features to avoid multicollinearity in model building (e.g., using PCA or dropping one of the correlated features).\n",
        "\n",
        "*   Focusing on features with stronger correlations with the target variable (Class) as they might be more predictive."
      ],
      "metadata": {
        "id": "_f7R-44Rktk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Violin plot for 'Amount' based on 'Class'\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.violinplot(x='Class', y='Amount', data=df)\n",
        "plt.title('Distribution of Transaction Amounts by Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WTjlii_Tyufl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin plot gives a deeper look into the distribution and density of transaction amounts by class. It shows that fraudulent transactions have a more concentrated distribution compared to legitimate ones, which are more spread out with outliers. Here we can explore additional features that might help differentiate frauds based on amount distributions, also handling outliers in the legitimate transactions to refine model performance."
      ],
      "metadata": {
        "id": "Dx_BwX2alZeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only keeping numeric features (skip target 'Class')\n",
        "X = df.drop(columns=['Class'])\n",
        "\n",
        "# Calculating VIF (Variance Inflation Factor) for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = X.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "guC9NDxgy1Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The VIF output indicates the level of multicollinearity among the features in dataset.\n",
        "\n",
        "Most features (e.g., \"V1\", \"V3\", \"V6\", \"V8\", and others) have VIF values close to 1, indicating very low multicollinearity. This suggests that these features are not significantly correlated with other features in the dataset, making them reliable for use in regression models.\n",
        "\n",
        "Features like \"V2\", \"V5\", \"V7\", and \"V20\" have VIF values between 2 and 4. These values are still within an acceptable range but indicate some degree of correlation with other features. While these values do not warrant immediate removal, it's important to monitor these features in case they lead to multicollinearity issues in your model.\n",
        "\n",
        "The \"Amount\" feature has a VIF value of approximately 11.5, which is quite high and indicates significant multicollinearity. This suggests that \"Amount\" may be highly correlated with one or more other features in the dataset. High VIF values like this could distort regression coefficients and affect the stability and interpretability of the model.\n",
        "\n",
        "Since Amount has a high VIF value, you might need to explore its correlation with other features. If it's highly correlated with other variables, we can removing it, if it doesn't contribute new information or combine it with other features or transform it to reduce the correlation.\n",
        "\n",
        "Although the VIF values between 2 and 4 are acceptable, it's a good idea to keep these features in mind when evaluating model performance, as they might introduce minor multicollinearity."
      ],
      "metadata": {
        "id": "jpby661FMgQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'Class' and compute summary statistics for continuous features\n",
        "grouped_stats = df.groupby('Class').mean()\n",
        "print(grouped_stats)"
      ],
      "metadata": {
        "id": "QKWsDl5xy_eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot of Amount grouped by Class\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.boxplot(x='Class', y='Amount', data=df)\n",
        "plt.title('Transaction Amount by Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aAqkjRJWzJWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This boxplot visualizes the distribution of transaction amounts for legitimate (Class 0) and fraudulent (Class 1) transactions. Fegitimate transactions (Class 0), the majority of transaction amounts are concentrated at lower values, with a few extreme outliers reaching up to over 25,000 and but  presence of numerous outliers suggests that legitimate transactions vary widely in amount, with most being small but some being very large.\n",
        "\n",
        "For fraudlent transactions (Class 1) show low fraudulent transaction but appear to be more tightly distributed, with significantly fewer extreme outliers compared to legitimate transactions. Which suggests that fraudulent transactions generally tend to have lower amounts, and there is less variation in their values."
      ],
      "metadata": {
        "id": "9QB_rGTU7Wpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "In this section, we will handle missing values, scale the data, and address class imbalance."
      ],
      "metadata": {
        "id": "KJphfztqvc3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicates\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "8l8ZLpT_N5_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if there are there still any duplicates?\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "76QUuZLz7iSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply scaling (excluding the target 'Class')\n",
        "df_scaled = df.copy()\n",
        "df_scaled.iloc[:, :-1] = scaler.fit_transform(df.iloc[:, :-1])\n",
        "\n",
        "# Verify scaling\n",
        "df_scaled.head()"
      ],
      "metadata": {
        "id": "SOQtcOr0l6pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output indicates that the values are scaled between 0 and 1, confirming that MinMaxScaler was applied correctly."
      ],
      "metadata": {
        "id": "ttQlvIQ-QA90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset contains substantial outliers, which is common in fraud detection, so RobustScaler is primary choice, because this scaler centers the data using the median and scales using the interquartile range (IQR), which makes it robust to outliers. So, to ensure that outliers are minimized first we will use  RobustScaler and then further normalize the data for algorithms sensitive to scaling StandardScaler.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u2IQDwKqFcqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RobustScaler first\n",
        "robust_scaler = RobustScaler()\n",
        "X_robust_scaled = robust_scaler.fit_transform(X)\n",
        "\n",
        "# Applying StandardScaler on the robust-scaled data\n",
        "standard_scaler = StandardScaler()\n",
        "X_final_scaled = standard_scaler.fit_transform(X_robust_scaled)"
      ],
      "metadata": {
        "id": "dpOO30qCFN4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a visualization technique which will use boxplots to compare the distribution of features (V17, V14, V12, V10) between the two classes (0 for non-fraud and 1 for fraud). It's particularly useful for understanding how these features correlate with the target class."
      ],
      "metadata": {
        "id": "M6IU-HsZD4HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming that df is our original DataFrame\n",
        "columns_needed = ['Class', 'V17', 'V14', 'V12', 'V10']\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "new_df = df[columns_needed]\n",
        "\n",
        "# Setting the color palette\n",
        "colors = ['#1f77b4', '#ff0000']\n",
        "\n",
        "# Creating a figure with 4 subplots in one row\n",
        "f, axes = plt.subplots(ncols=4, figsize=(20, 4))\n",
        "\n",
        "# Plotting boxplot for each feature against Class\n",
        "sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\n",
        "axes[0].set_title('V17 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\n",
        "axes[1].set_title('V14 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\n",
        "axes[2].set_title('V12 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\n",
        "axes[3].set_title('V10 vs Class Negative Correlation')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TTw-mjNEsywI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variables displayed in this plot (e.g., V17, V14, V12, and V10) show strong negative correlations with the target class. This indicates that as the values of these variables increase, the likelihood of fraud decreases.\n",
        "\n",
        "We need to:\n",
        "\n",
        "*   Incorporate these variables prominently in the model as they demonstrate strong predictive potential.\n",
        "\n",
        "*   Use feature engineering to create interaction terms or non-linear transformations if these variables exhibit non-linear relationships with the target.\n"
      ],
      "metadata": {
        "id": "3DYijusMmDXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a correlation matrix with imbalanced and SubSample data\n",
        "fraud = df[df['Class'] == 1]\n",
        "non_fraud = df[df['Class'] == 0].sample(n=len(fraud), random_state=42)\n",
        "\n",
        "# Concatenate fraud and non-fraud samples to create a balanced subsample\n",
        "subsample = pd.concat([fraud, non_fraud])\n",
        "\n",
        "# Compute correlation matrices\n",
        "corr_matrix_full = df.corr()\n",
        "corr_matrix_subsample = subsample.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8, 6))\n",
        "\n",
        "# Imbalanced correlation matrix\n",
        "sns.heatmap(corr_matrix_full, cmap='coolwarm', ax=ax[0], cbar_kws={'shrink': 0.5}, vmin=-1, vmax=1)\n",
        "ax[0].set_title('Imbalanced Correlation Matrix\\n(don\\'t use for reference)', fontsize=16)\n",
        "\n",
        "# Subsample correlation matrix\n",
        "sns.heatmap(corr_matrix_subsample, cmap='coolwarm', ax=ax[1], cbar_kws={'shrink': 0.5}, vmin=-1, vmax=1)\n",
        "ax[1].set_title('SubSample Correlation Matrix\\n(use for reference)', fontsize=6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6XWNsAD6v6bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two correlation matrices illustrate the relationship between different features in the dataset, showing how preprocessing, especially subsampling, impacts the representation and correlations between features.\n",
        "\n",
        "Imbalanced correlaition matrix show the  lacks variability and detail, indicating that the class imbalance masks true relationships between features. This is because the overwhelming majority of non-fraudulent (Class 0) data points dominate the calculation, making the correlation values less reliable for model development. This matrix is not ok to  be used as a reference for feature selection or understanding relationships since it is skewed by the imbalance.\n",
        "\n",
        "SubSample correlation matrix shows a more diverse pattern of correlations, with a mixture of positive and negative correlations across different feature pairs. This suggests that the subsampled dataset is better balanced, allowing the true relationships to surface. There is a\n",
        "strong correlations between some features (e.g., between V3, V6, and V9) which could indicate multicollinearity. These features might need to be addressed using techniques like Principal Component Analysis (PCA) or feature elimination to reduce redundancy.\n",
        "\n"
      ],
      "metadata": {
        "id": "p0OvA9b3A3Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumeing df is the dataset and features are the continuous features that we apply PCA on\n",
        "features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "X = df[features]\n",
        "\n",
        "# First standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Now applying PCA\n",
        "pca = PCA(n_components=0.95)  # adjusting the \"n_components\" parameter in PCA accordingly.\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Visualizing the explained variance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance by Components')\n",
        "plt.show()\n",
        "\n",
        "# Transforming the data\n",
        "# The transformed dataset (X_pca) now has reduced dimensions based on the number of components chosen\n",
        "pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
        "\n",
        "# Adding the target variable back to the transformed dataset if needed\n",
        "pca_df['Class'] = df['Class']\n",
        "\n",
        "# Displaying the first few rows of the PCA-transformed dataset\n",
        "print(pca_df.head())\n"
      ],
      "metadata": {
        "id": "jPLiB4WCHTTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Principal Component Analysis (PCA) plot illustrates how the cumulative variance is explained as more principal components are added. In this plot, the variance steadily increases with each additional component, eventually reaching close to 100% when all components are included. This indicates that PCA effectively captures the variance present in the original dataset, allowing us to reduce the dataset's dimensionality while retaining most of the important information. We select the number of components that capture around 95% of the variance, which strikes a balance between dimensionality reduction and preserving the dataset's integrity.\n",
        "\n",
        "By examining the PCA output, it's clear that the principal components (e.g., PC1, PC2, etc.) are new, uncorrelated dimensions that summarize the original features. These components can be used for further model training, reducing noise and multicollinearity. The explained variance plot guides us in determining the optimal number of components to retain, ensuring that we maintain a high proportion of the dataset’s variance while improving model efficiency and performance. This dimensionality reduction process is crucial for simplifying the dataset, making it more manageable and suitable for building predictive models.\n"
      ],
      "metadata": {
        "id": "bqd0e7-ZHvV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 _ Logistic Regression"
      ],
      "metadata": {
        "id": "dEhwsmMoRb6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'X_pca' is the PCA-transformed feature set and 'y' is the target variable, df is the original dataset\n",
        "y = df['Class']\n",
        "\n",
        "# Spliting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing the logistic regression model, adjusting max_iter if necessary for convergence\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Training the model using the training data\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "y_pred_prob = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Calculating AUC\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"Logistic Regression Model Accuracy:\", accuracy)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "print(\"\\nAUC Score:\", auc)"
      ],
      "metadata": {
        "id": "c4liHU_kajuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 2 _ Random Forest Classifier"
      ],
      "metadata": {
        "id": "_8g5nULQRg6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RandomForestClassifier with class weights to handle imbalance\n",
        "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "\n",
        "# Fitting the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ZLLD1zVoy_Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access one of the trees in the forest (e.g., the first tree)\n",
        "tree = rf_model.estimators_[0]  # Accessing the first tree in the forest\n",
        "\n",
        "# Visualizing the tree\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_tree(tree,\n",
        "          filled=True,\n",
        "          feature_names=X_train.columns if isinstance(X_train, pd.DataFrame) else ['Feature_' + str(i) for i in range(X_train.shape[1])],\n",
        "          class_names=['Not Fraud', 'Fraud'],\n",
        "          rounded=True)\n",
        "plt.title('Decision Tree Visualization (Tree 1 of Random Forest)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HXJdIACujSt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision tree visualization provided represents one of the individual trees from the Random Forest model built to classify transactions as either fraudulent or non-fraudulent. The tree is composed of several levels where splits are made based on specific feature values, with each node representing a decision point. The tree uses these splits to divide the dataset into smaller subsets, attempting to group similar observations together. The decision rules at each node are determined based on features that maximize information gain, measured by a reduction in Gini impurity, which indicates how mixed the classes are within a node. A Gini value of 0 at a node means that all samples belong to a single class, making the node pure.\n",
        "\n",
        "As the tree progresses down each path, it uses different features, such as Feature_1, Feature_14, and Feature_12, suggesting that these attributes are influential in distinguishing between fraudulent and non-fraudulent transactions. Early splits in the tree rely on these critical features, helping the model make initial decisions about which direction the transaction should proceed. The tree then continues to branch out further using other features, each split further refining the classification until the leaf nodes are reached. These leaf nodes represent the final classification for the observations, where the \"class\" value shows whether the majority of samples at that node are classified as fraud or not fraud. It’s notable that some branches become pure (Gini = 0) quite quickly, indicating a clear separation of classes based on certain feature values, while other branches require more depth to achieve purity.\n",
        "\n",
        "This individual tree is just one part of the entire Random Forest, which is composed of many such trees, each capturing different aspects of the data. While this single tree provides insight into how certain features and their values influence the classification process, it is important to remember that the forest, as a whole, averages the predictions from all these trees, reducing the risk of overfitting. Overfitting is a concern when trees grow too deep and fit noise in the training data rather than capturing general patterns. However, the Random Forest model mitigates this by aggregating results across multiple trees, leading to a more robust and generalizable model. This visualization allows us to understand the decision-making process within the forest and highlights the importance of certain features in classifying fraudulent versus non-fraudulent transactions."
      ],
      "metadata": {
        "id": "2QlAWN_7zwKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 3 _ SVM Model"
      ],
      "metadata": {
        "id": "tBlUvWTZRoKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming that X_scaled is the scaled feature set and y is the target variable\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing the SVM model with class weights to handle class imbalance\n",
        "svm_model = SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)\n",
        "\n",
        "# Training the model using the training data\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Getting prediction probabilities for AUC calculation\n",
        "y_pred_prob = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Calculating AUC\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"SVM Model Accuracy:\", accuracy)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "print(\"\\nAUC Score:\", auc)"
      ],
      "metadata": {
        "id": "Xp6S7TdloyW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 4 _ XGBOOST"
      ],
      "metadata": {
        "id": "2Z6n-SwlBCz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'X' and 'y' are your scaled and processed features and target variable\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing XGBoost with scale_pos_weight to handle class imbalance\n",
        "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, scale_pos_weight=scale_pos_weight)\n",
        "\n",
        "# Fitting the model to the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "y_pred_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Calculating AUC\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"XGBoost Model Accuracy:\", accuracy)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "print(\"\\nAUC Score:\", auc)"
      ],
      "metadata": {
        "id": "udzUC-mD7_E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ROC Curves for All The Models"
      ],
      "metadata": {
        "id": "qPTFhHueHtlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the models\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "svm_model = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Fitting models on training data and get probabilities for ROC\n",
        "models = {\n",
        "    'Logistic Regression': log_reg,\n",
        "    'Random Forest': rf_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'SVM': svm_model\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for Multiple Models')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Whp29PmCHdh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC curve visualization compares the performance of four models.\n",
        "\n",
        "**Logistic Regression (AUC = 0.98):** This model performs very well, with an AUC (Area Under the Curve) of 0.98. The ROC curve is close to the top-left corner, indicating that the model has a high true positive rate and a low false positive rate. It is a near-optimal model for this dataset.\n",
        "\n",
        "**Random Forest (AUC = 1.00):** The Random Forest model achieves a perfect AUC of 1.00, which indicates separation between classes. The ROC curve follows the axes precisely, meaning it correctly identifies all true positives and negatives without any error. This model provides the best fit among all.\n",
        "\n",
        "**XGBoost (AUC = 1.00):** Similar to Random Forest, the XGBoost model also achieves a perfect AUC of 1.00. The curve again touches the top-left corner, demonstrating that the model perfectly distinguishes between fraudulent and non-fraudulent cases. Both Random Forest and XGBoost models perform exceptionally well and are likely the best choices for this dataset.\n",
        "\n",
        "**SVM (AUC = 0.45):** The SVM model, with an AUC of 0.45, performs poorly compared to the others. Its ROC curve is below the diagonal line, indicating that it performs worse than random guessing. This suggests that the SVM model is not suitable for this particular dataset or problem without further tuning or feature engineering."
      ],
      "metadata": {
        "id": "D0LYJNw8MJvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating The Models"
      ],
      "metadata": {
        "id": "UXIZizjEI_9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an empty list to store results\n",
        "results = []\n",
        "\n",
        "# Training and evaluating each model\n",
        "for model_name, model in models.items():\n",
        "    # Fitting the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Making predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probability for the positive class (Fraud)\n",
        "\n",
        "    # Calculating metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "    # Append results to the list\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'ROC AUC': auc\n",
        "    })\n",
        "\n",
        "# Converting results to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Displaying the results\n",
        "display(df_results)"
      ],
      "metadata": {
        "id": "I2HSO8RFJvs9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}